# -*- coding: utf-8 -*-
"""rand3d.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LnZDn0EFsGFZh65ZPK1yNgHuzr7baz4b
"""

from warnings import filterwarnings
filterwarnings('ignore')

import numpy as np
import pandas as pd
import seaborn as sns
from glob import glob
import matplotlib.pyplot as plt
from tqdm.auto import tqdm as tqdm

from fastai.vision import *
from fastai.metrics import error_rate

images = [plt.imread(f"Scenes/rand3d_" + str(i).zfill(2) + "_" + str(j).zfill(4) + ".png") for i in np.arange(1) for j in np.arange(100)]

fig, axes = plt.subplots(2, 3, sharex=True, sharey=True) 
for img, ax in zip(images[:6], axes.flat):
    ax.imshow(img)
    ax.axis('off')

assets = glob('Scenes/*.png')

dictlist = []
label_counts = {}
for asset in assets:
    shape = int(asset[14:16])
    view = int(asset[17:21])
    row = {'image_name': asset, 'image_shape': shape, 'image_view': view}
    dictlist.append(row)
label_df = pd.DataFrame(dictlist)

label_df

model_options = {'vgg16': models.vgg16_bn, 'vgg19': models.vgg19_bn, 'resnet18': models.resnet18, 'resnet34': models.resnet34,
             'resnet50': models.resnet50, 'resnet152': models.resnet152}

get_transforms()

path = Path('') # define path to our images 

data = (ImageImageList.from_df(label_df, path, cols='image_name') # specify the column with the image names
            .split_none()
            .label_from_func(lambda x: x) # notice! the label for our image is the image itself
            .transform(tfm_y = True, size = 224) # apply a resize transform
            .databunch(bs=48)).normalize(imagenet_stats) # define batch size and normalize with imagenet statistics

data

#@title Run this cell to load the autoencoder architecture.

# this code is based heavily on the following github repo: 
# https://github.com/nadavbh12/VQ-VAE/blob/master/vq_vae/auto_encoder.py

import abc
import torch
from torch import nn
from torch.nn import functional
from torch.autograd import Variable
import torch.nn.functional as F
import torchvision.models


class AbstractAutoEncoder(nn.Module):
    __metaclass__ = abc.ABCMeta

    @abc.abstractmethod
    def encode(self, x):
        return

    @abc.abstractmethod
    def decode(self, z):
        return

    @abc.abstractmethod
    def forward(self, x):
        """model return (reconstructed_x, *)"""
        return

    @abc.abstractmethod
    def sample(self, size):
        """sample new images from model"""
        return

    @abc.abstractmethod
    def loss_function(self, **kwargs):
        """accepts (original images, *) where * is the same as returned from forward()"""
        return

    @abc.abstractmethod
    def latest_losses(self):
        """returns the latest losses in a dictionary. Useful for logging."""
        return

class ResBlock(nn.Module):
    def __init__(self, in_channels, channels, bn=False):
        super(ResBlock, self).__init__()

        layers = [
            nn.ReLU(),
            nn.Conv2d(in_channels, channels, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.Conv2d(in_channels, channels, kernel_size=1, stride=1, padding=0)]
        if bn:
            layers.insert(2, nn.BatchNorm2d(channels))
        self.convs = nn.Sequential(*layers)

    def forward(self, x):
        return x + self.convs(x)

#Adversarial Convolutional Variational Autoencoder
class ACVAE(AbstractAutoEncoder):
    def __init__(self, input_shape, input_channels=3, latent_dim=128, **kwargs):
        super(ACVAE, self).__init__()
        
        print("Adversarial VAE Model with Resnet Encoder & Latent Space of Size ", latent_dim)
        
        features = torchvision.models.resnet18(pretrained=True)
        self.encoder=nn.Sequential(*(list(features.children())[:-2]))
        
        self.decoder = nn.Sequential(
            ResBlock(input_shape, input_shape, bn=True),
            nn.BatchNorm2d(input_shape),
            ResBlock(input_shape, input_shape, bn=True),
            nn.BatchNorm2d(input_shape),
            
            nn.ConvTranspose2d(input_shape, input_shape, kernel_size=3, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(input_shape),
            nn.ReLU(inplace=True),

            nn.ConvTranspose2d(input_shape, input_shape // 2, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(input_shape//2),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(input_shape // 2, 32, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            
            nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(16),
            nn.ReLU(inplace=True),
            
            nn.ConvTranspose2d(16, input_channels, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(input_channels),
            nn.ReLU(inplace=True),
            
            nn.ConvTranspose2d(input_channels, input_channels, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(input_channels),   
        )
        
        self.input_shape = input_shape
        #25088 corresponds to the flattened dimensions of the last conv layer of Resnet18
        self.fc_mu = nn.Linear(25088,latent_dim)
        self.fc_std = nn.Linear(25088,latent_dim)
        self.fc_z = nn.Linear(latent_dim,input_shape*4**2)        
    
        self.mse = 0

    def encode(self, x):
        h1 = self.encoder(x)
        h1 = h1.view(-1, 25088)
        return self.fc_mu(h1), self.fc_std(h1)

    def reparameterize(self, mu, logvar):
        std = logvar.mul(0.5).exp_()
        eps = Variable(std.new(std.size()).normal_())
        return eps.mul(std).add_(mu)

    def decode(self, z):
        z=self.fc_z(z)
        z = z.view(-1, self.input_shape, 4, 4)
        h3 = self.decoder(z)
        return torch.tanh(h3)

    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        self.mu, self.logvar = mu, logvar
        return self.decode(z)#, mu, logvar
    
    def get_latent(self,x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        return z
        
    def sample(self, size):
        sample = Variable(torch.randn(size, 128), requires_grad=False)
        if self.cuda():
            sample = sample.cuda()
        return self.decode(sample).cpu()
    
    def loss_function(self, x, recon_x):
        self.mse = F.mse_loss(recon_x, x)
        self.kl_loss = -0.5 * torch.sum(1 + self.logvar - self.mu.pow(2) - self.logvar.exp())
        self.kl_loss /= x.size(0) * 3 * 1024
        return 0.5 * self.mse + 1.0 * self.kl_loss

    def latest_losses(self):
        return {'mse': self.mse}

vae = ACVAE(224, input_channels=3) # first, we instantiate our vae model

# we then pass this to a fastai learner object to facilitate training
# reference: https://towardsdatascience.com/autoencoders-in-the-fastai-library-fa288e1f899a
learner = Learner(data, vae, opt_func=torch.optim.Adam, loss_func=vae.loss_function)

# notice in fastai that our custom model is stored directly in the learner object
# vae == learn.model evaluates to True

learner.lr_find()

learner.recorder.plot(suggestion=True)
min_grad_lr = learner.recorder.min_grad_lr

learner.fit_one_cycle(cyc_len = 10, max_lr=1e-04)

